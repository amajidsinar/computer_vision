{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "class HDF5DatasetWriter:\n",
    "    \"\"\"\n",
    "    The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n",
    "The dims parameter controls the dimension or shape of the data we will be storing in the dataset. Think of dims as the .shape of a NumPy array. If we were storing the (flattened) raw pixel intensities of the 28 × 28 = 784 MNIST dataset, then dims=(70000, 784) as there are 70,000 examples in MNIST, each with a dimensionality of 784. If we wanted to store the raw CIFAR-10 images, then we would set dims=(60000, 32, 32, 3) as there are 60,000 total images in the CIFAR-10 dataset, each represented by a 32 × 32 × 3 RGB image.\n",
    "\n",
    "In the context of transfer learning and feature extraction, we’ll be using the VGG16 architecture and taking the outputs after the final POOL layer. The output of the final POOL layer is 512 × 7 × 7 which, when flattened, yields a feature vector of length 25,088. Therefore, when using VGG16 for feature extraction, we’ll set dims=(N, 25088) where N is the total number of images in our dataset.\n",
    "\n",
    "The next parameter to the HDF5DatasetWriter constructor is the outputPath – this is the path to where our output HDF5 file will be stored on disk. The optional dataKey is the name of the dataset that will store the data our algorithm will learn from. We default this value to \"images\", since in most cases we’ll be storing raw images in HDF5 format. However, for this example, when we instantiate the HDF5DatasetWriter we’ll set dataKey=\"features\" to indicate that we are storing features extracted from a CNN in the file.\n",
    "\n",
    "Finally, bufSize controls the size of our in-memory buffer, which we default to 1,000 feature vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dims, outputPath, dataKey=\"images\", bufSize=1000):\n",
    "        # check to see if the output path exists, and if so, raise an exception\n",
    "        if os.path.exists(outputPath):\n",
    "            raise ValueError(\"The supplied 'outputPath' already \"\n",
    "                             \"exists and cannot be overwritten. Manually delete \"\n",
    "                             \"the file before continuing. \", outputPath)\n",
    "            \n",
    "        # In general, this will open the HDF5 database for writing \n",
    "        # and create two datasets:\n",
    "        # one to store the images/features and another to store the\n",
    "        # class labels\n",
    "        \n",
    "        # opens the HDF5 file for writing using the supplied outputPath\n",
    "        self.db = h5py.File(outputPath, \"w\")\n",
    "        \n",
    "        # create dataset with the dataKey name and supplied dims\n",
    "        self.data = self.db.create_dataset(dataKey, dims, dtype=\"float\")\n",
    "        # create dataset to store the class labels\n",
    "        self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")\n",
    "        \n",
    "        # store the buffer size, then initialize the buffer itself\n",
    "        # along with the index into the datasets\n",
    "        self.bufSize = bufSize\n",
    "        self.buffer = {\"data\": [], \"labels\": []}\n",
    "        self.idx = 0\n",
    "        \n",
    "    def add(self, rows, labels):\n",
    "        # add the rows and labels to the buffer\n",
    "        self.buffer[\"data\"].extend(rows)\n",
    "        self.buffer[\"labels\"].extend(labels)\n",
    "        \n",
    "        # check to see if the buffer needs to be flushed to disk\n",
    "        if len(self.buffer[\"data\"]) >= self.bufSize:\n",
    "            self.flush()\n",
    "        \n",
    "    def flush(self):\n",
    "        # in general, this will write the buffers to disk then reset the buffer\n",
    "        \n",
    "        # determine the next available row in the matrix\n",
    "        i = self.idx + len(self.buffer[\"data\"])\n",
    "        # apply slicing to store the data\n",
    "        self.data[self.idx:i] = self.buffer[\"data\"]\n",
    "        # apply slicing to storethe label\n",
    "        self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
    "        self.idx = i\n",
    "        # reset the buffer\n",
    "        self.buffer = {\"data\": [], \"labels\": []}\n",
    "        \n",
    "    def storeClassLabels(self, classLabels):\n",
    "        # store the raw string names of the class labels in a separate dataset\n",
    "        dt = h5py.special_dtype(vlen=bytes)\n",
    "        labelSet = self.db.create_dataset(\"label_names\", (len(classLabels),), dtype=dt)\n",
    "        labelSet[:] = classLabels\n",
    "        \n",
    "    def close(self):\n",
    "        # check to see if there are any other entries in the buffer\n",
    "        # that need to be flushed to disk\n",
    "        if len(self.buffer[\"data\"]) > 0:\n",
    "               self.flush()\n",
    "               \n",
    "        # close the dataset\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n",
    "The dims parameter controls the dimension or shape of the data we will be storing in the dataset. Think of dims as the .shape of a NumPy array. If we were storing the (flattened) raw pixel intensities of the 28 × 28 = 784 MNIST dataset, then dims=(70000, 784) as there are 70,000 examples in MNIST, each with a dimensionality of 784. If we wanted to store the raw CIFAR-10 images, then we would set dims=(60000, 32, 32, 3) as there are 60,000 total images in the CIFAR-10 dataset, each represented by a 32 × 32 × 3 RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [3000,200]\n",
    "outputPath = \"test/5.hdf5\"\n",
    "dataKey = \"X\"\n",
    "buffSize = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = h5py.File(outputPath, \"w\")\n",
    "predictor = db.create_dataset(\"predictor\", dims, dtype=float)\n",
    "response = db.create_dataset(\"response\", (dims[0],1), dtype=int)\n",
    "buffSize = buffSize \n",
    "buffer = {\"X\" :[], \"y\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input arrays to the buffer\n",
    "\n",
    "buffer[\"X\"].extend(np.ones((5,200)))\n",
    "buffer[\"y\"].extend(np.ones((5,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer[\"X\"].extend(np.ones((10,200))*2)\n",
    "buffer[\"y\"].extend(np.ones((10,1))*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buffer[\"X\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush buffer into the hdf5\n",
    "\n",
    "predictor[0:len(buffer[\"X\"])] = buffer[\"X\"]\n",
    "response[0:len(buffer[\"y\"])] = buffer[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = h5py.File(outputPath, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['predictor', 'response']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(read.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "xe = read[\"predictor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "ye = read[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"predictor\": shape (3000, 200), type \"<f8\">"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 200)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 1)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"predictor\": shape (3000, 200), type \"<f8\">"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ye[6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
